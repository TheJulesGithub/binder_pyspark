{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1fe32d3",
   "metadata": {},
   "source": [
    "# BE 1: Introduction\n",
    "\n",
    "This first notebook aims to present some very high level features of `PySpark`. \n",
    "\n",
    "Here are topics: \n",
    "* Initiate SpakSession\n",
    "* Create and manipulate PySpark DataFrames - lazy computation\n",
    "* Reading a \"big\" file and manipulate data\n",
    "* Writting results\n",
    "\n",
    "This first BE is done on a Jupyter Notebook. Note that you shall use this IDE only for debugging / testing your app, not for industrial application.\n",
    "\n",
    "\n",
    "## 1/ Recap on Spark and distributed computation\n",
    "\n",
    "Pyspark documentation can be found [here](https://spark.apache.org/docs/latest/api/python/getting_started/index.html).\n",
    "\n",
    "\n",
    "\n",
    "### 1.1/ Cluster\n",
    "\n",
    "This schema recap distribution of data between: master / workers\n",
    "\n",
    "![img/spark-cluster-overview](img/spark-cluster-overview.png)\n",
    "\n",
    "*NB:* For this BE, everything will be done on the same physical machine (we will \"imagine\" that we have access to numerous workers comming from several physical machine). All the `Pyspark` commands are the same in a cluster.\n",
    "\n",
    "### 1.2/ Lazy computation\n",
    "\n",
    "When Spark transforms data, it does not immediately compute the transformation but plans how to compute later. When actions such as `collect()` are explicitly called, the computation starts. This notebook shows the basic usages of the DataFrame, geared mainly for new users.\n",
    "\n",
    "All actions can be found [here](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions).\n",
    "\n",
    "Note that `show()` is an action. \n",
    "\n",
    "**Code**\n",
    "\n",
    "Follow the BE: just execute the cells and see results.\n",
    "\n",
    "*Tips*: Use Ctrl + Enter to execute a cell.\n",
    "\n",
    "**Imports**\n",
    "\n",
    "List here all your imports (good practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b34916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a SparkSession:\n",
    "from pyspark.sql import SparkSession\n",
    "# Time windowing\n",
    "from pyspark.sql.window import Window\n",
    "# To manipulate time data\n",
    "from datetime import datetime, date\n",
    "# To manipulate \"not distrubuted DataFrames\"\n",
    "import pandas as pd\n",
    "# To manipulate \"Distributed DataFrames\"\n",
    "from pyspark.sql import Row\n",
    "# String into upper \n",
    "import pyspark.sql.functions as F\n",
    "# Manage time (especially store computing time)\n",
    "from time import time\n",
    "# Access to System command and variables\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d52b0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environ var\n",
    "# ----------------------------------------------\n",
    "# Just two set-up command to make it work.\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# '/opt/rh/rh-python38/root/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4bc5a-838f-45db-8432-105d69dff015",
   "metadata": {},
   "source": [
    "## 2/ Create a `SparkSession`\n",
    "\n",
    "`PySpark` applications start with initializing `SparkSession` which is the entry point of `PySpark` as below. \n",
    "\n",
    "*NB*: In case of running it in `PySpark shell` (command line) via pyspark executable, the shell automatically creates the session in the variable spark for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345269d8-5c4f-4eb3-8f35-ffac9e3d0227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/11/17 16:16:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a90f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://juphub-prod01.tsoc.fr.astrium.corp:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f09422338e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd8aa41-d56c-4977-8469-5f9f0a7a6026",
   "metadata": {},
   "source": [
    "## 3/ Exercice 1: word count\n",
    "\n",
    "Let's start with a log file. We work on a toy example of log file (source [here](https://www.ibm.com/docs/en/zos/2.2.0?topic=problems-example-log-file)).\n",
    "\n",
    "This exercice contains:\n",
    "* Reading a text file\n",
    "* lines / words count\n",
    "* filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2f0e1-e90d-47cc-8f93-7e20578a6dd4",
   "metadata": {},
   "source": [
    "### 3.1/ Usefull commands\n",
    "\n",
    "**Read text file**\n",
    "\n",
    "Let's see what is the content of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c817e5d-a1bf-41c5-bb40-d007cc4b3893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open ‘text.txt’ for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Print the 5 first lines of the file to read (Linux command `head -n 5 <file>`)\n",
    "!head -n 5 text.txt\n",
    "# NB: use `!<OS command>` to use an OS command on Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430d75fc-0457-469c-abc0-2dca3080767c",
   "metadata": {},
   "source": [
    "This is a structured file containing multiples lines:\n",
    "* lines with date + log (e.g. `03/22 08:51:01 INFO   :...locate_configFile: Specified configuration file: /u/user10/rsvpd1.conf`)\n",
    "* lines with a two digits number (e.g. `01`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8151da7-9011-46ed-86c8-949e22bbf84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a text file lines by lines\n",
    "lines=spark_session.sparkContext.textFile('data/text.txt')\n",
    "# ex: [' 01 ', '03/22 08:51:01 INFO   :.main: *************** RSVP Agent started ***************', ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bbd5ef-3137-4be0-b837-33da4adf3adb",
   "metadata": {},
   "source": [
    "Everything is stored in an RDD ([Resilient Distributed Dataset](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)), meaning that the data is distributed between workers per *lines* (one block per workers, multiple lines per block).\n",
    "\n",
    "*NB:* RDD is not used directly in `PySpark` as the latest versions propose `DataFrame` objects, easier to manipulate (see next Exercice)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf93f867-55e8-4b21-9ba3-ed72df5d7f1b",
   "metadata": {},
   "source": [
    "**Count lines**\n",
    "\n",
    "Command `count()` permits to count the number of lines. It is an ACTION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "823e7127-31d0-4feb-9d68-49146ad42d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count each line of the file (ACTION)\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d754e93-c0ed-4a40-9da4-ecdb3a802a52",
   "metadata": {},
   "source": [
    "**Count every word**\n",
    "\n",
    "Here is a short script to count each word occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa417386-fd63-4e48-8836-e0ba1cd86505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faltten (1 word by 'line')\n",
    "words = lines.flatMap(lambda line: line.split(' '))\n",
    "# ex: ['', '01', '', '03/22',...]\n",
    "\n",
    "# Create a \"key- value pairing\" (key=word, value=1)\n",
    "words_with_1 = words.map(lambda word: (word, 1))\n",
    "# ex: [[('', 1), ('01', 1), ('', 1), ('03/22', 1),...]\n",
    "\n",
    "# Sum all \"values\" having the same \"keys\" (same word)\n",
    "word_counts = words_with_1.reduceByKey(lambda count1, count2: count1 + count2)\n",
    "# Ex: [('', 104), ('03/22', 52), ...]\n",
    "\n",
    "# Launch process and retrieve result (ACION)\n",
    "result = word_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c65f0963-118f-4313-9bd7-44d7ca827d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word ``: 104\n",
      "Word `03/22`: 52\n",
      "Word `INFO`: 41\n",
      "Word `:.main:`: 2\n",
      "Word `***************`: 2\n",
      "Word `RSVP`: 13\n",
      "Word `Agent`: 1\n",
      "Word `started`: 1\n",
      "Word `:...locate_configFile:`: 1\n",
      "Word `configuration`: 1\n"
     ]
    }
   ],
   "source": [
    "# Print the count of each word\n",
    "for (word, count) in result[:10]:  # only the first 10\n",
    "    print(f\"Word `{word}`: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe4405b-2144-44a2-a4c2-e6de8136ba07",
   "metadata": {},
   "source": [
    "**Filter**\n",
    "\n",
    "Use `filter(<filter function>)` to filter your objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a985e842-f012-493d-9d11-2ae84400d5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['03/22 08:51:06 INFO   :....mailslot_create: creating mailslot for timer',\n",
       " '03/22 08:51:06 INFO   :...mailbox_register: mailbox allocated for timer']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter all object in `lines` (lines of the file) containing word \"time\"\n",
    "lines.filter(lambda l:  \"timer\" in l).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd884649-1a70-405c-95f3-a978575e6c13",
   "metadata": {},
   "source": [
    "*Action performed:*\n",
    "* iterate on each lines `l` of the text\n",
    "    * test if word \"timer\" is in the current line\n",
    "    * if it's the case, store the line\n",
    "\n",
    "\n",
    "*Explanation*: \n",
    "* `lambda`: create a python \"lambda function\": a function defined in one line\n",
    "* `l`: input of the function (here content of the line, e.g. `'03/22 08:51:06 INFO   :....mailslot_create: creating mailslot for timer'`\n",
    "* `\"timer\" in l`: a condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ec748-b11e-4473-a019-39f0e67f781c",
   "metadata": {},
   "source": [
    "**Map: apply a function to each elements**\n",
    "\n",
    "Use `map(<function>)` to apply a`<function>` to each element of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "832ae900-3fe0-4a23-a1dd-9e9b421d5963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' 01 '],\n",
       " ['03',\n",
       "  '22 08:51:01 INFO   :.main: *************** RSVP Agent started ***************'],\n",
       " [' 02 '],\n",
       " ['03',\n",
       "  '22 08:51:01 INFO   :...locate_configFile: Specified configuration file: ',\n",
       "  'u',\n",
       "  'user10',\n",
       "  'rsvpd1.conf'],\n",
       " ['03', '22 08:51:01 INFO   :.main: Using log level 511'],\n",
       " ['03',\n",
       "  '22 08:51:01 INFO   :..settcpimage: Get TCP images rc - EDC8112I Operation not supported on socket.'],\n",
       " [' 03 '],\n",
       " ['03',\n",
       "  '22 08:51:01 INFO   :..settcpimage: Associate with TCP',\n",
       "  'IP image name = TCPCS'],\n",
       " ['03',\n",
       "  '22 08:51:02 INFO   :..reg_process: registering process with the system'],\n",
       " ['03', '22 08:51:02 INFO   :..reg_process: attempt OS', '390 registration']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: split each lines by \"/\"\n",
    "r = lines.map(lambda l: l.split(\"/\")).collect()\n",
    "\n",
    "r[:10]  # only discplay the 10 first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011abf44-56c5-4795-a47d-80625ab653fd",
   "metadata": {},
   "source": [
    "### 3.2/ Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa187245-6c3f-44d7-84cf-d72485f154dc",
   "metadata": {},
   "source": [
    "**Exercice: filter and count all lines containing \"WARNING\" (uppercase)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36fc1baa-19e6-4ac6-9cc2-10f71eb9a5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['03/22 08:51:06 WARNING:.....mailslot_create: setsockopt(MCAST_ADD) failed - EDC8116I Address not available.',\n",
       " '03/22 08:51:06 WARNING:.....mailslot_create: setsockopt(MCAST_ADD) failed - EDC8116I Address not available.',\n",
       " '03/22 08:51:06 WARNING:.....mailslot_create: setsockopt(MCAST_ADD) failed - EDC8116I Address not available.',\n",
       " '03/22 08:51:06 WARNING:.....mailslot_create: setsockopt(MCAST_ADD) failed - EDC8116I Address not available.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only keep lines with at least on word contains \"WARNING\"\n",
    "lines.filter(lambda l: \"WARNING\" in l).collect()\n",
    "# Ex: ['03/22 08:51:06 WARNING:.....mailslot_create: setsockopt(MCAST_ADD) failed - EDC8116I Address not available.', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff6ad887-fe5b-40d2-9c74-080d27813088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer:\n",
    "lines.filter(lambda l: \"WARNING\" in l).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fdf5cf-eb58-45ee-8b40-5df5d4523509",
   "metadata": {},
   "source": [
    "**Exercice: count all lines containing only two digits**\n",
    "\n",
    "E.g. ` 01 `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e677dad-f42c-48d6-b4c3-149aab48f399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 01 ', ' 02 ', ' 03 ', ' 04 ', ' 05 ', ' 06 ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Structure of these lines: \" xx \". => ned to consider all lines starting with space \" \".\n",
    "lines.filter(lambda l: l[0] == \" \").collect()\n",
    "# ex: [' 01 ', ' 02 ', ' 03 ', ' 04 ', ' 05 ', ' 06 ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d8dcb2c-009c-4280-8d89-5edba60223fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer:\n",
    "lines.filter(lambda l: l[0] == \" \").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5ee78f-4907-4891-9006-447a95e5be8c",
   "metadata": {},
   "source": [
    "**Exercice: drop `\"<date> <log level>:\"` from each lines**\n",
    "    \n",
    "Example: \n",
    "`'03/22 08:51:06 WARNING:.....mailslot_create: setsockopt(MCAST_ADD) failed - EDC8116I Address not available.'`\n",
    "\n",
    "into\n",
    "\n",
    "`'.....mailslot_create: setsockopt(MCAST_ADD) failed - EDC8116I Address not available.'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42f8dfbe-5565-49fb-aacd-b1cfe027ef74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['main: *************** RSVP Agent started ***************'],\n",
       " [],\n",
       " ['..locate_configFile: Specified configuration file: /u/user10/rsvpd1.conf'],\n",
       " ['main: Using log level 511'],\n",
       " ['.settcpimage: Get TCP images rc - EDC8112I Operation not supported on socket.'],\n",
       " [],\n",
       " ['.settcpimage: Associate with TCP/IP image name = TCPCS'],\n",
       " ['.reg_process: registering process with the system'],\n",
       " ['.reg_process: attempt OS/390 registration']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer:\n",
    "lines.map(lambda l: l.split(\":.\")[1:]).collect()[:10]\n",
    "\n",
    "# Explanation: \n",
    "# * For each line (`l`)\n",
    "# * split line by \":.\"\n",
    "# * drop the first element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33529068-f53c-44eb-bc6b-e17507c6fec7",
   "metadata": {},
   "source": [
    "## 3/ Exercice 2: `DataFrame` manipulation\n",
    "\n",
    "A PySpark DataFrame can be created via `pyspark.sql.SparkSession.createDataFrame` typically by passing a list of `pyspark.sql.Rows`. \n",
    "\n",
    "\n",
    "Firstly, you can create a PySpark DataFrame from a list of 3 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22bb84eb-8b08-46f3-8d27-8b4285e1d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99869c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7543e0",
   "metadata": {},
   "source": [
    "`print()` only indicates the columns names and types. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec0480-d839-4ca9-aed2-071528846ae0",
   "metadata": {},
   "source": [
    "**Schema**\n",
    "\n",
    "`pyspark.sql.SparkSession.createDataFrame` takes the schema argument to specify the schema of the DataFrame. When it is omitted, PySpark infers the corresponding schema by taking a sample from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2d845f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5bca53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a846dda",
   "metadata": {},
   "source": [
    "**Show**\n",
    "\n",
    "Lets see what contains the `df` DataFrame, with `show()` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2010072a-acb9-4af3-bf15-ece898cdef9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f0115",
   "metadata": {},
   "source": [
    "Note that:\n",
    "* `show()` is an action (explanation below).\n",
    "* by default, `show()` only display the 20 first rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33789de",
   "metadata": {},
   "source": [
    "### 3.1/ `DataFrame` commands\n",
    "\n",
    "We can manipulate `DataFrames` as tables, with:\n",
    "* columns selection (`select()`)\n",
    "* filters (`filter()`)\n",
    "* Column creation (`withColumns()`)\n",
    "* simple operations (`avg`, `min`, `max`, ...)\n",
    "* ordering (`orderBy()`)\n",
    "* any SQL command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f4171",
   "metadata": {},
   "source": [
    "**Column selection**\n",
    "\n",
    "`select()` permits you to select one or multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fd9742b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|2.0|\n",
      "|  2|3.0|\n",
      "|  3|4.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select cols `a` and `b`\n",
    "df_ab = df.select(\"a\", \"b\")\n",
    "\n",
    "# Show result\n",
    "df_ab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34276f3b",
   "metadata": {},
   "source": [
    "You can chain commands in Pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2de1b778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|2.0|\n",
      "|  2|3.0|\n",
      "|  3|4.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select columns \"a\" and \"b\" AND show result (No dataFrame is created here)\n",
    "df.select(\"a\", \"b\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd2fbdb",
   "metadata": {},
   "source": [
    "**Column filtering**\n",
    "\n",
    "To select a subset of rows, use `filter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc5faf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select rows where col \"a\" = 1\n",
    "df_a = df.filter(df.a == 1)\n",
    "\n",
    "# Show result\n",
    "df_a.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae93cc",
   "metadata": {},
   "source": [
    "*NB*: Use `df.a` command inside a formula to refer to column \"a\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d369f382",
   "metadata": {},
   "source": [
    "**Column creation**\n",
    "\n",
    "Store result of a past operation in a new column with `withColumns()` command.\n",
    "\n",
    "Usage:\n",
    "`withColumn(<new column name>, <column content (formula)>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc56d662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  a|  b|      c|         d|                  e|upper_c|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|STRING2|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|STRING3|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new column containing \"c\" column into upper.\n",
    "df_uc = df.withColumn('upper_c', F.upper(df.c))\n",
    "\n",
    "df_uc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665635c",
   "metadata": {},
   "source": [
    "You can also update a column with this command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94212eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|3.0|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|5.0|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|7.0|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update column \"a\" with \"a+b\"\n",
    "df.withColumn('a', df.a + df.b).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28958fb7",
   "metadata": {},
   "source": [
    "*NB*: Find [here](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#module-pyspark.sql.functions) the list of available functions (source: `pyspark.sql.functions` library)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7047c232",
   "metadata": {},
   "source": [
    "**Simple operations on groups**\n",
    "\n",
    "PySpark DataFrame also provides a way of handling grouped data by using the common approach, split-apply-combine strategy.\n",
    "It groups the data by a certain condition applies a function to each group and then combines them back to the DataFrame.\n",
    "\n",
    "Here is an example on a new DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c6a4792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  1| 10|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|carrot|  3| 30|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|black|carrot|  6| 60|\n",
      "|  red|banana|  7| 70|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fruits = spark_session.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "df_fruits.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c77cf8",
   "metadata": {},
   "source": [
    "Grouping and then applying the `avg()` function to the resulting groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "813e9fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|color|avg(v1)|avg(v2)|\n",
      "+-----+-------+-------+\n",
      "|  red|    4.8|   48.0|\n",
      "| blue|    3.0|   30.0|\n",
      "|black|    6.0|   60.0|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fruits.groupby('color').avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d68c204-4f49-4813-8150-72eb3d957c2b",
   "metadata": {},
   "source": [
    "You can also apply a function (`avg`, `min`, `max`, ...) on all the `Rows` using `select(<function>(<columns_name>))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2481ad92-b40a-4adc-968b-44443cab0592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|min(v1)|\n",
      "+-------+\n",
      "|      1|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fruits.select(F.min('v1')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b73960-115a-4574-96df-c7d5a5d0c336",
   "metadata": {},
   "source": [
    "To access to the value (here `1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52894c0d-690b-44ae-aeb1-e42a4c189c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store in intermediate var `v`\n",
    "v = df_fruits.select(F.min('v1'))\n",
    "\n",
    "v.collect()[0]['min(v1)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee750e-1669-4179-8076-1f071d70a16c",
   "metadata": {},
   "source": [
    "**Ordering results**\n",
    "\n",
    "Use `OrderBy(<column_name>)` to sort DataFrame by <column_name>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acab9b1c-51db-481a-8747-2ad0709061b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "| blue|banana|  2| 20|\n",
      "|  red|banana|  1| 10|\n",
      "|  red|banana|  7| 70|\n",
      "|  red|carrot|  5| 50|\n",
      "|  red|carrot|  3| 30|\n",
      "|black|carrot|  6| 60|\n",
      "|  red| grape|  8| 80|\n",
      "| blue| grape|  4| 40|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort `df_fruits.fruits` column in alphabetical order\n",
    "df_fruits.orderBy(\"fruit\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa53a8",
   "metadata": {},
   "source": [
    "**Using SQL command**\n",
    "\n",
    "DataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. For example, you can register the DataFrame as a table and run a SQL easily as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b18e226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       8|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# `df_fruits` will be referenced as \"tableA\" in our SQL command\n",
    "df_fruits.createOrReplaceTempView(\"tableA\")\n",
    "\n",
    "# Count table rows\n",
    "spark_session.sql(\"SELECT count(*) from tableA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538f4a36",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2/ Exercice\n",
    "\n",
    "You will then read a \"big\" file with Spark, and turn it into a SparkDataFrame. You will manipulate the file data.json containing some data of 10k agents in the word (fake data). (source: https://s3-eu-west-1.amazonaws.com/course.oc-static.com/courses/4297166/agents.json). Here is an example of the 3 thrist lines of this file:\n",
    "\n",
    "```\n",
    "{\"id\":227417393,\"longitude\":100.85840672174572,\"latitude\":33.15219798270325,\"country_name\":\"China\",\"sex\":\"Male\"}\n",
    "{\"id\":6821129477,\"longitude\":-72.43795260265814,\"latitude\":19.325567983697297,\"country_name\":\"Haiti\",\"sex\":\"Female\"}\n",
    "{\"id\":2078667700,\"longitude\":80.85636526088884,\"latitude\":23.645271492037235,\"country_name\":\"India\",\"sex\":\"Female\"}\n",
    "```\n",
    "\n",
    "This kind of data perfectly fit the SparkDataFrame format:\n",
    "* Columns: corresponding to the fields (id, longitude, ...)\n",
    "* Multiples Rows, one row per agent.\n",
    "\n",
    "Note that Rows are distributed between nodes (the columns are not distributed) in a SparkDataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c21c56",
   "metadata": {},
   "source": [
    "**Read a file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f55a6fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+-------------------+------+\n",
      "|        country_name|        id|          latitude|          longitude|   sex|\n",
      "+--------------------+----------+------------------+-------------------+------+\n",
      "|               China| 227417393| 33.15219798270325| 100.85840672174572|  Male|\n",
      "|               Haiti|6821129477|19.325567983697297| -72.43795260265814|Female|\n",
      "|               India|2078667700|23.645271492037235|  80.85636526088884|Female|\n",
      "|               China| 477556555| 33.45864668881662|  93.33604038078953|Female|\n",
      "|               India|1379059984|28.816938290678692|   80.7728698035823|Female|\n",
      "|               India|2278934249|24.223974351280358|  80.14372690674512|  Male|\n",
      "|         Philippines|4380736204|12.409991630883784| 122.75874146810197|Female|\n",
      "|               India|1375733494|22.385712662257426|  77.90320433636231|Female|\n",
      "|             Nigeria|3693807307| 9.967458870426357|  7.562942449523648|Female|\n",
      "|                Mali|6552202234|16.882575147323337| -3.949079041016242|Female|\n",
      "|            Thailand|5057542320|13.501463223632582| 103.05981834176352|  Male|\n",
      "|               India|1806935868|  18.4459850484637|  74.28181188756791|  Male|\n",
      "|               China| 265404548|29.447948266668902| 106.56441719305467|Female|\n",
      "|               China|1272365006|35.450817753169304| 106.05970256662503|  Male|\n",
      "|            Pakistan|3427393483|29.930865126652122|  70.15711512834791|Female|\n",
      "|Central African R...|7029557128|  7.01006931921262| 21.005933006443826|Female|\n",
      "|               Haiti|6826219119|18.940384961447766| -72.32688350214578|  Male|\n",
      "|            Tanzania|5357138381|-5.983046108539422|  34.80683011234468|  Male|\n",
      "|               Ghana|6186906042| 7.969120948577613|-1.9820137477590403|Female|\n",
      "|              Brazil|3275490483|-9.425790844213656| -55.88198203024498|  Male|\n",
      "+--------------------+----------+------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.read.json(\"data/agents.json\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ec17c",
   "metadata": {},
   "source": [
    "**Exercise 1: Count French male person**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09d702bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer\n",
    "df.filter(df.country_name == \"France\").filter(df.sex==\"Male\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d486a3f-3d15-4b03-b834-1df430a01ef4",
   "metadata": {},
   "source": [
    "**Exercice 2: Count the `Male` and `Female` per country name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "026ad612-355c-4f8c-b039-e4bdf3762fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-----+\n",
      "|country_name|   sex|count|\n",
      "+------------+------+-----+\n",
      "| Afghanistan|Female|   32|\n",
      "| Afghanistan|  Male|   17|\n",
      "|     Albania|  Male|    2|\n",
      "|     Albania|Female|    5|\n",
      "|     Algeria|Female|   25|\n",
      "|     Algeria|  Male|   29|\n",
      "|      Angola|Female|   15|\n",
      "|      Angola|  Male|   13|\n",
      "|   Argentina|Female|   34|\n",
      "|   Argentina|  Male|   18|\n",
      "|     Armenia|  Male|    1|\n",
      "|     Armenia|Female|    3|\n",
      "|   Australia|  Male|   15|\n",
      "|   Australia|Female|   18|\n",
      "|     Austria|Female|    9|\n",
      "|     Austria|  Male|    7|\n",
      "|  Azerbaijan|  Male|    9|\n",
      "|  Azerbaijan|Female|    9|\n",
      "|     Bahrain|  Male|    2|\n",
      "|  Bangladesh|Female|  109|\n",
      "+------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "df.groupby(\"country_name\", \"sex\").count().orderBy(\"country_name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24755379-4974-4bf7-9c3b-8b90eb259047",
   "metadata": {},
   "source": [
    "**Exercice 3: find the maximum available `latitude` between all the Chinese `Male` personn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3421b472-5a88-41f9-a5b7-96f844496561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134.55941926197372"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer\n",
    "df.filter(df.country_name == \"China\").filter(df.sex == \"Male\").select(F.max(\"longitude\")).collect()[0]['max(longitude)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d497d",
   "metadata": {},
   "source": [
    "## 4/ Exercice 3: Manipulate time series / anomaly detection\n",
    "\n",
    "In this exercice, you will manipulate timestamped data. \n",
    "\n",
    "*Data decription*:\n",
    "* Data came from `data.csv` file\n",
    "* File containing 2 time series:\n",
    "    * Timestamp column ('time')\n",
    "    * orbit id: the id of the current orbit (10 orbits in the file)\n",
    "    * tm_1: values of time serie 1\n",
    "    * tm_2: value of time series 2\n",
    "    \n",
    "These data contains anomaly of 3 kinds: holes, shifts and spikes.\n",
    "\n",
    "### 4.1/ Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d84771-8bc4-4c0f-93bf-0323efcef006",
   "metadata": {},
   "source": [
    "**Read data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96e6124c-de19-4023-ac2d-a6a9ef330231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time,orbit_id,tm_1,tm_2\n",
      "11/16/2022 15:26:47,0,0,1\n",
      "11/16/2022 15:27:47,0,1,5\n",
      "11/16/2022 15:28:47,0,6,2\n",
      "11/16/2022 15:29:47,0,0,3\n"
     ]
    }
   ],
   "source": [
    "# Linux command to display the 5 first line of the file\n",
    "!head -n 5 data/data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e4828f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a CSV file containing data, here, file contains a header indicating column names\n",
    "df_spark = spark_session.read.options(header=True).csv(\"data/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0dcfe-dfb6-4b0e-9c59-ed9b4813d722",
   "metadata": {},
   "source": [
    "Let's see the schema of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cf440eb-140b-4043-a287-31f95d755087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[time: string, orbit_id: string, tm_1: string, tm_2: string]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1931a3-492e-428c-8f0b-18e4385a1d16",
   "metadata": {},
   "source": [
    "Ah... `tm_1` and `tm_2` shall be float, and `orbit_id` shall be integer.\n",
    "\n",
    "**Exercice: Re-read file with correct format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b947d173-2936-4303-8ce7-66b3dc71e9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[time: string, orbit_id: int, tm_1: int, tm_2: int]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a CSV file containing data, here, file contains a header indicating column names\n",
    "df_spark = spark_session.read.options(header=True, schema='time date, orbit_id integer, tm_1 double, tm_2 double').csv(\"data/data.csv\")\n",
    "df_spark = spark_session.read.options(header=True, inferSchema=True).csv(\"data/data.csv\")\n",
    "\n",
    "df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "343dafcd-d86a-4994-84a6-ee9fbfe5fcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+----+----+\n",
      "|               time|orbit_id|tm_1|tm_2|\n",
      "+-------------------+--------+----+----+\n",
      "|11/16/2022 15:26:47|       0|   0|   1|\n",
      "|11/16/2022 15:27:47|       0|   1|   5|\n",
      "|11/16/2022 15:28:47|       0|   6|   2|\n",
      "|11/16/2022 15:29:47|       0|   0|   3|\n",
      "|11/16/2022 15:30:47|       0|   1|   2|\n",
      "|11/16/2022 15:31:47|       0|   3|   8|\n",
      "|11/16/2022 15:32:47|       0|   4|   4|\n",
      "|11/16/2022 15:33:47|       0|   1|   9|\n",
      "|11/16/2022 15:34:47|       0|   4|   2|\n",
      "|11/16/2022 15:35:47|       0|   3|   5|\n",
      "|11/16/2022 15:36:47|       0|   0|   1|\n",
      "|11/16/2022 15:37:47|       0|   5|   0|\n",
      "|11/16/2022 15:38:47|       0|   4|   3|\n",
      "|11/16/2022 15:39:47|       0|   2|   3|\n",
      "|11/16/2022 15:40:47|       0|   2|   9|\n",
      "|11/16/2022 15:41:47|       0|   8|   1|\n",
      "|11/16/2022 15:42:47|       0|   2|   4|\n",
      "|11/16/2022 15:43:48|       0|   6|   9|\n",
      "|11/16/2022 15:44:48|       0|   3|   7|\n",
      "|11/16/2022 15:45:48|       0|   5|   6|\n",
      "+-------------------+--------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541efec9",
   "metadata": {},
   "source": [
    "**Find holes**\n",
    "\n",
    "Some part of the data contains gaps. You have to find it using `Window` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c5daa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current window: ALL the timestamps (1 single window)\n",
    "my_window = Window.orderBy('time').partitionBy('orbit_id')\n",
    "# `artitionBy`: the blocks of the RDD are stored by `orbit_id`.\n",
    "\n",
    "# INPUT: Df with col `time` and other columns with TM values (not used)\n",
    "# OUTPUT: Same with new col `delay_col`\n",
    "# PROCESS: Compute column `time` with lag (get time-1)\n",
    "df_spark = df_spark.withColumn(\"delay_col\", F.lag(\"time\").over(my_window))\n",
    "# Note: `null` value on first value of `delay_col`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "190a8f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+----+----+-------------------+\n",
      "|               time|orbit_id|tm_1|tm_2|          delay_col|\n",
      "+-------------------+--------+----+----+-------------------+\n",
      "|11/16/2022 15:26:47|       0|   0|   1|               null|\n",
      "|11/16/2022 15:27:47|       0|   1|   5|11/16/2022 15:26:47|\n",
      "+-------------------+--------+----+----+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c53e6767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+----+----+-------------------+-------------+\n",
      "|               time|orbit_id|tm_1|tm_2|          delay_col|diff_time_col|\n",
      "+-------------------+--------+----+----+-------------------+-------------+\n",
      "|11/16/2022 15:26:47|       0|   0|   1|               null|         null|\n",
      "|11/16/2022 15:27:47|       0|   1|   5|11/16/2022 15:26:47|         null|\n",
      "+-------------------+--------+----+----+-------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enable legacy mode for date parsing\n",
    "spark_session.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "# INPUT: Df with col `time`, `t-1` and other columns with TM values (not used)\n",
    "# OUTPUT: Same with new col `diff_time_col` containing lag between timesteps\n",
    "# PROCESS: Calculate delay (t - [t-1])\n",
    "df_spark = df_spark.orderBy(\"time\").withColumn(\"diff_time_col\", (F.unix_timestamp(\"time\") - F.unix_timestamp(\"delay_col\")))\n",
    "\n",
    "df_spark.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c08aac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply threshold\n",
    "# -------------------------------------------------------\n",
    "hole_th = 10\n",
    "# Threshold in seconds\n",
    "hole_threshold = hole_th * 60  # in seconds\n",
    "\n",
    "# INPUT: Df with col `time`, `delay_col`, `diff_time_col` and other columns with TM values (not used)\n",
    "# OUTPUT: Same with all rows where `diff_time_col`> threshold\n",
    "# PROCESS: Filter rows > threshold (corresponds to hole)\n",
    "df_hole_spark = df_spark.filter(F.column(\"diff_time_col\") > hole_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45ff9f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----+----+---------+-------------+\n",
      "|time|orbit_id|tm_1|tm_2|delay_col|diff_time_col|\n",
      "+----+--------+----+----+---------+-------------+\n",
      "+----+--------+----+----+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hole_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce430e6-9744-4456-a8d9-33aa0fa6f27f",
   "metadata": {},
   "source": [
    "**PartitionBy Explained**\n",
    "\n",
    "\n",
    "Display the two firsts times of each of the 10 orbits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9abd3fd8-6530-4d31-9b01-d3aa084b5456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+----+----+-------------------+-------------+---+\n",
      "|               time|orbit_id|tm_1|tm_2|          delay_col|diff_time_col|row|\n",
      "+-------------------+--------+----+----+-------------------+-------------+---+\n",
      "|11/16/2022 15:26:47|       0|   0|   1|               null|         null|  1|\n",
      "|11/16/2022 15:27:47|       0|   1|   5|11/16/2022 15:26:47|         null|  2|\n",
      "|11/16/2022 18:07:54|       1|   9|   8|               null|         null|  1|\n",
      "|11/16/2022 18:08:54|       1|   7|   0|11/16/2022 18:07:54|         null|  2|\n",
      "|11/16/2022 20:14:59|       2|   7|  16|               null|         null|  1|\n",
      "|11/16/2022 20:15:59|       2|   4|   0|11/16/2022 20:14:59|         null|  2|\n",
      "|11/16/2022 22:39:05|       3|   6|   2|               null|         null|  1|\n",
      "|11/16/2022 22:40:05|       3|   5|   0|11/16/2022 22:39:05|         null|  2|\n",
      "|11/17/2022 01:19:12|       4|   4|   4|               null|         null|  1|\n",
      "|11/17/2022 01:20:12|       4|   8|   7|11/17/2022 01:19:12|         null|  2|\n",
      "|11/17/2022 03:27:17|       5|   9|   9|               null|         null|  1|\n",
      "|11/17/2022 03:28:17|       5|   0|   0|11/17/2022 03:27:17|         null|  2|\n",
      "|11/17/2022 05:51:23|       6|   3|   8|               null|         null|  1|\n",
      "|11/17/2022 05:52:23|       6|   4|   6|11/17/2022 05:51:23|         null|  2|\n",
      "|11/17/2022 08:15:29|       7|   6|   0|               null|         null|  1|\n",
      "|11/17/2022 08:16:29|       7|   4|   1|11/17/2022 08:15:29|         null|  2|\n",
      "|11/17/2022 10:39:35|       8|   4|   4|               null|         null|  1|\n",
      "|11/17/2022 10:40:35|       8|   7|   3|11/17/2022 10:39:35|         null|  2|\n",
      "|11/17/2022 14:47:45|       9|   7|   1|               null|         null|  1|\n",
      "|11/17/2022 14:48:45|       9|   9|   9|11/17/2022 14:47:45|         null|  2|\n",
      "+-------------------+--------+----+----+-------------------+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new col `row` containing the row id\n",
    "# do that over `myWindo` (partitioned by \"orbit_id\")\n",
    "# Filter only the two first cols\n",
    "df_spark.withColumn(\"row\", F.row_number().over(my_window)).filter(F.col(\"row\") <= 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797a373-a692-4d01-9c72-88e166867972",
   "metadata": {},
   "source": [
    "*NB:* Note that there is 10 times `null` value !\n",
    "\n",
    "> Explaination: \n",
    ">* You have partitioned by `orbit_id`, there is 10 orbits in the Dataset\n",
    ">* The delay is computed between times foreach partition\n",
    ">* between partition k and k+1, no comparison > delay is `null` for the first "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf2d0c",
   "metadata": {},
   "source": [
    "**Write results**\n",
    "\n",
    "Results are stored in a directory `foo.csv` containing a `csv` concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad45d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results in a CSV file\n",
    "df_spark.write.csv('foo.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9aec2-1b1d-4080-8075-9b6062279504",
   "metadata": {},
   "source": [
    "### 4.2/ Execrice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b0639-ca79-45b3-9365-e07098316d8c",
   "metadata": {},
   "source": [
    "**Exercice 1: Get the count of each hole per duration**\n",
    "\n",
    "Expecting a result like this: \n",
    "\n",
    "```\n",
    "+----------------+-----+\n",
    "|<hole duration> |count|\n",
    "+----------------+-----+\n",
    "|              61|   44|\n",
    "|            4323|    1|\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "651c0fb4-66ae-4530-ba4c-456369f62987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|diff_time_col|count|\n",
      "+-------------+-----+\n",
      "|         null| 1024|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "df_spark.groupBy('diff_time_col').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f57056",
   "metadata": {},
   "source": [
    "**Exercice 2: find anomaly of type 'spike' on `tm_2`**\n",
    "\n",
    "The dataset contains very high values due to sensor anomaly. \n",
    "\n",
    "You have to filter it with a simple threshold method.\n",
    "\n",
    "![img/spike](img/spike.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a7a2101-393c-40ed-8413-9c4ad319587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold\n",
    "THRESH = [-1, 20]  # min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0bebfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               time|\n",
      "+-------------------+\n",
      "|11/17/2022 10:05:33|\n",
      "|11/17/2022 10:06:33|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ANSWER: for TM 1\n",
    "df_spike = df_spark.filter((F.col('tm_2') > THRESH[1]) | (F.col('tm_2') < THRESH[0]))\n",
    "\n",
    "# Only keep time col\n",
    "df_spike = df_spike.select('time')\n",
    "\n",
    "df_spike.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e618f9-e124-4ffa-89cb-71bfbbff6b04",
   "metadata": {},
   "source": [
    "**Exercice 3: find the orbit containing a shift**\n",
    "\n",
    "More complex, the time serie 2 (`tm_2`) contains shifts on a certain orbit. Find them:\n",
    "\n",
    "* Use a windowing avegaring to detect high shifts (window on each orbit).\n",
    "* Think to filter the spikes, that could affect the average value\n",
    "* An \"abnormal\" (up to you to define it) value shall raise.\n",
    "\n",
    "![img/shift.png](img/shift.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b9ae464-0773-4587-933e-b33a5f63cfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|orbit_id|         avg(tm_2)|\n",
      "+--------+------------------+\n",
      "|       1| 4.559055118110236|\n",
      "|       6| 4.680555555555555|\n",
      "|       3|  4.36144578313253|\n",
      "|       5| 4.597222222222222|\n",
      "|       9|             3.875|\n",
      "|       4|            4.4375|\n",
      "|       8| 4.119047619047619|\n",
      "|       7| 4.674242424242424|\n",
      "|       2|              9.75|\n",
      "|       0|4.2894736842105265|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Answer:\n",
    "\n",
    "# 1/ Drop spike \n",
    "# ---------------------\n",
    "# Create a column containing `True` if spike detected\n",
    "df_spark = df_spark.withColumn(\"is_spike\", (F.col('tm_2') > THRESH[1]) | (F.col('tm_2') < THRESH[0]))\n",
    "\n",
    "df_spark_without_spike = df_spark.filter(~ F.col(\"is_spike\"))\n",
    "\n",
    "# 2/ Get the mean per orbit, and compare values\n",
    "# --------------------------------------------------\n",
    "df_spark_without_spike.groupby(\"orbit_id\").mean(\"tm_2\").show()\n",
    "\n",
    "# Orbit 9 have a clear anomaly (to investigate by experts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2bb672-a953-47c7-a92c-8d8d1066797c",
   "metadata": {},
   "source": [
    "Orbit 2 is clearly the one containing anomalies !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b1779f8-c995-449a-9cc7-64a149aefc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|orbit_id|         avg(tm_2)|\n",
      "+--------+------------------+\n",
      "|       1| 4.559055118110236|\n",
      "|       6| 4.680555555555555|\n",
      "|       3|  4.36144578313253|\n",
      "|       5| 4.597222222222222|\n",
      "|       9|             3.875|\n",
      "|       4|            4.4375|\n",
      "|       8| 4.119047619047619|\n",
      "|       7| 1497.141791044776|\n",
      "|       2|              9.75|\n",
      "|       0|4.2894736842105265|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Considering spikes\n",
    "df_spark.groupby(\"orbit_id\").mean(\"tm_2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4396b2",
   "metadata": {},
   "source": [
    "## 5/ Exercice 4: optimize your workflow  \n",
    "\n",
    "**Lazy computation explaination**: \n",
    "* `spark_session.createDataFrame(...)` create a dataFrame with multiple `Row` object\n",
    "* These rows are *distributed* between the nodes (ex: Row 1 & 2 goes to node 1, Row 2 and 3 goes to node 2)\n",
    "* All *transformation* (like `filter()`, `select()`, ...) are not directly done. The command is stored by the `TaskManager` (\"lazy\" computation)\n",
    "* These commands are executed only when an *action* (like `show()`, `count()`, ...) is requested.\n",
    "\n",
    "At the execution:\n",
    "* 1/ The TaskManager optimize the tasks, it will re-organise the transformation requested (e.g. aggregating multiple filters, ...) to accelerate the computation time. At this stage, nothing is done.\n",
    "* 2/ The TaskManager send commands to each nodes. These nodes execute in paralell the optimal workflow provided by the TaskManager\n",
    "* 3/ All results are aggregated in the Master node. The final result is then available. In general, the data transfert create a bottleneck in your workflow. Use at minimal *transformations* to avoid calling multiple time this data transfert !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2152d-bdd5-478a-a23a-4b735128eb49",
   "metadata": {},
   "source": [
    "**Excercice: optimize the workflow of the previous exercice**\n",
    "\n",
    "* Use `cache()` to store intermediate results\n",
    "* Store results in separated CSV:\n",
    "    * holes\n",
    "    * spikes\n",
    "    * orbit containing shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43abca68-34d1-4aa4-9e70-5bc57d5d07e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
