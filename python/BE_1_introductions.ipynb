{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a4b9005",
   "metadata": {},
   "source": [
    "# BE 1\n",
    "\n",
    "Assumption: \n",
    "* Pyspark is installed\n",
    "* Java is installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5769e033",
   "metadata": {},
   "source": [
    "## Create a \"session\"\n",
    "\n",
    "A `SparkSession` is a session running your Spark application. A `SparkSession` objects will permit you to manipulate usefull functions (read/write). \n",
    "\n",
    "You shall run an unique `SparkSession` per project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af65e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649e5c4",
   "metadata": {},
   "source": [
    "## Read a file\n",
    "\n",
    "You will then read a \"big\" file with Spark, and turn it into a `SparkDataFrame`.\n",
    "\n",
    "You will manipulate the file `data.json` containing some data of 10k agents in the word (fake data). (source: https://s3-eu-west-1.amazonaws.com/course.oc-static.com/courses/4297166/agents.json). Here is an example of the 3 thrist lines of this file:\n",
    "\n",
    "```\n",
    "{\"id\":227417393,\"longitude\":100.85840672174572,\"latitude\":33.15219798270325,\"country_name\":\"China\",\"sex\":\"Male\"}\n",
    "{\"id\":6821129477,\"longitude\":-72.43795260265814,\"latitude\":19.325567983697297,\"country_name\":\"Haiti\",\"sex\":\"Female\"}\n",
    "{\"id\":2078667700,\"longitude\":80.85636526088884,\"latitude\":23.645271492037235,\"country_name\":\"India\",\"sex\":\"Female\"}\n",
    "```\n",
    "This kind of data perfectly fit the `SparkDataFrame` format:\n",
    "* Columns: corresponding to the fields (id, longitude, ...)\n",
    "* Multiples `Rows`, one row per agent. \n",
    "\n",
    "Note that `Rows` are distributed between nodes (the columns are not distributed) in a `SparkDataFrame`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eaa1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read as DF\n",
    "df_spark = spark_session.read.json(\"data.json\")\n",
    "\n",
    "df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3601b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c006abca",
   "metadata": {},
   "source": [
    "## Into Pandas\n",
    "\n",
    "Not distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into pandas df\n",
    "df_pandas = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c1581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# Instantiation d'un SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27a8566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture d'un fichier texte : le fichier est décomposé en lignes.\n",
    "lines = sc.textFile(\"data.json\")\n",
    "\n",
    "# Décomposition de chaque ligne en mots\n",
    "word_counts = lines.flatMap(lambda line: line.split(' '))\n",
    "\n",
    "# Chacun des mots est transformé en une clé-valeur\n",
    "word_counts = word_counts.map(lambda word: (word, 1))\n",
    "\n",
    "\n",
    "# Les valeurs associées à chaques clé sont sommées\n",
    "word_counts = word_counts.reduceByKey(lambda count1, count2: count1 + count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le résultat est récupéré\n",
    "word_count = word_counts.collect()\n",
    "\n",
    "# Chaque paire (clé, valeur) est affichée\n",
    "for (word, count) in word_counts:\n",
    "    print(word, count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
